{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7Ee3PT89BKYO8UnjZgRVR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldsober-irene/NLP-fellowship/blob/main/scrap_igihe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R-XR_MEXHWJ5"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from time import strftime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create the sequencial (daily) links from january to december 2022"
      ],
      "metadata": {
        "id": "DHkXxZUMK-iG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validLinks = []\n",
        "current_month = int(strftime(\"%m\")) + 1\n",
        "for month in range(1, current_month):\n",
        "  for day in range(1, 32):\n",
        "    l = \"http://archive.org/wayback/available?url=igihe.com&timestamp=2022{:02d}{:02d}\".format(month, day)\n",
        "    response = requests.get(l).json()\n",
        "    if  response['archived_snapshots']:\n",
        "      validLinks.append(response['archived_snapshots']['closest'][\"url\"])"
      ],
      "metadata": {
        "id": "veKDVcJsIq1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check if they are some duplicated links and get rid of them"
      ],
      "metadata": {
        "id": "V79q-kk-Lng9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array(validLinks)\n",
        "series = pd.Series(arr)\n",
        "validLinks = series.unique()"
      ],
      "metadata": {
        "id": "L0IrLhxBLu5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_links = {}"
      ],
      "metadata": {
        "id": "q90Jo_R8SUmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get every article link and its date of creation"
      ],
      "metadata": {
        "id": "1SDUJSITSryb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_links(link):\n",
        "  page = requests.get(link)\n",
        "  soup = bs(page.text, \"html.parser\")\n",
        "  \n",
        "  b = soup.body\n",
        "  get_titles = [link+re.sub(\"amakuru/\",\"\", a['href']) for span in b.find_all(\"span\", class_ = \"homenews-title\") for a in span.find_all(\"a\")]\n",
        "  \n",
        "  # get only links whose date is in current year\n",
        "  pattern = re.compile(r\"\\d{8,}?\")\n",
        "  found_date = pattern.search(link).group()\n",
        "  if found_date[3] != str(2):\n",
        "    pass\n",
        "  else:\n",
        "    file_title = f\"igihe-{found_date[0:4]}-{found_date[4:6]}-{found_date[6:8]}\"\n",
        "    try:\n",
        "      for title in get_titles:\n",
        "        article_links[file_title].append(title)\n",
        "    except KeyError:\n",
        "      article_links[file_title] = []\n",
        "      for title in get_titles:\n",
        "        article_links[file_title].append(title)"
      ],
      "metadata": {
        "id": "OXQIXJetSrEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for link in validLinks:\n",
        "  process_links(link)"
      ],
      "metadata": {
        "id": "jLEqQl7vTeaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function to write article's paragraphs to a file corresponding to the article's creation date"
      ],
      "metadata": {
        "id": "dnEY2DasTo2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def writeToFile(paragraphs, file):\n",
        "  with open(file, \"a+\") as f:\n",
        "    for paragraph in paragraphs:\n",
        "      f.write(paragraph)"
      ],
      "metadata": {
        "id": "DPO_VFBcTi_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a function to grab the article url and return the article's text i.e. paragraphs and call the function to write those paragraphs to their corresponding file"
      ],
      "metadata": {
        "id": "oQj4Ng4ZURkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page():\n",
        "  count = 1\n",
        "  all_file = list(article_links.keys())\n",
        "  for key in list(article_links.keys()):\n",
        "    for link in set(article_links[key]):\n",
        "      p = requests.get(link).content\n",
        "      soup = bs(p, \"html.parser\")\n",
        "      spans = soup.find_all(\"div\", class_= \"fulltext margintop10\")\n",
        "      paragraphs = [p.get_text() for span in spans for p in span.find_all(\"p\")]\n",
        "      writeToFile(paragraphs = paragraphs, file = f\"/content/drive/MyDrive/NLP fellowship/week1 tasks/{key}.txt\")\n",
        "    # \n",
        "    print(f\"{count}file(s) created \\tremains : {all_file - count} files\")\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "eNfcY2bWUnax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "finish the program here"
      ],
      "metadata": {
        "id": "-3eqVzpiVdVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FINISH THE PROGRAM\n",
        "get_page()"
      ],
      "metadata": {
        "id": "1whzXQbyVgrY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}