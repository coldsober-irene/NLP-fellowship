{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coldsober-irene/NLP-fellowship/blob/main/week1Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ASSIGNMENT II** : scrap igihe.com from January to present"
      ],
      "metadata": {
        "id": "07W3ZYr0bF39"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r6fCO2XrPkbO"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import re\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijuHFYUdPEY-",
        "outputId": "04220ad7-2eac-4214-e457-ad0b1f1f81ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unsortedLinks = []\n",
        "\n",
        "for month in range(1, 11):\n",
        "  for day in range(1, 32):\n",
        "    l = \"http://archive.org/wayback/available?url=igihe.com&timestamp=2022{:02d}{:02d}\".format(month, day)\n",
        "    response = requests.get(l).json()\n",
        "    if  response['archived_snapshots']:\n",
        "      unsortedLinks.append(response['archived_snapshots']['closest'][\"url\"])"
      ],
      "metadata": {
        "id": "uAeHmrCaRE9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(unsortedLinks)"
      ],
      "metadata": {
        "id": "wazMetsxUIXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GET ALL ARTICLE AVAILABLE IN EVERY LINK\n"
      ],
      "metadata": {
        "id": "pTZ0eoxTXz6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array(unsortedLinks)\n",
        "series = pd.Series(arr)\n",
        "unsortedLinks = series.unique()"
      ],
      "metadata": {
        "id": "aaObZfxtV7y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "serious_links = {}\n"
      ],
      "metadata": {
        "id": "9SVdzehDhrCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# link = \"https://web.archive.org/web/20220615035616/https://www.igihe.com/amakuru/\"\n",
        "def process_links(link):\n",
        "  page = requests.get(link)\n",
        "  soup = bs(page.text, \"html.parser\")\n",
        "  \n",
        "  b = soup.body\n",
        "  get_titles = [link+re.sub(\"amakuru/\",\"\", a['href']) for span in b.find_all(\"span\", class_ = \"homenews-title\") for a in span.find_all(\"a\")]\n",
        "  \n",
        "  # get only links whose date is in current year\n",
        "  pattern = re.compile(r\"\\d{8,}?\")\n",
        "  found_date = pattern.search(link).group()\n",
        "  if found_date[3] != str(2):\n",
        "    pass\n",
        "  else:\n",
        "    file_title = f\"igihe-{found_date[0:4]}-{found_date[4:6]}-{found_date[6:8]}\"\n",
        "    try:\n",
        "      for title in get_titles:\n",
        "        serious_links[file_title].append(title)\n",
        "    except KeyError:\n",
        "      serious_links[file_title] = []\n",
        "      for title in get_titles:\n",
        "        serious_links[file_title].append(title)"
      ],
      "metadata": {
        "id": "RWBqtAaGnqEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for link in unsortedLinks:\n",
        "  process_links(link)"
      ],
      "metadata": {
        "id": "vN79F78QK76k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "serious_links"
      ],
      "metadata": {
        "id": "-H6yZC9fMRQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writeToFile(paragraphs, file):\n",
        "  with open(file, \"a+\") as f:\n",
        "    for line in paragraphs:\n",
        "      f.write(line)"
      ],
      "metadata": {
        "id": "3tAWz2wTOIPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page():\n",
        "  count = 1\n",
        "  for key in list(serious_links.keys()):\n",
        "    for link in set(serious_links[key]):\n",
        "      p = requests.get(link).content\n",
        "      soup = bs(p, \"html.parser\")\n",
        "      spans = soup.find_all(\"div\", class_= \"fulltext margintop10\")\n",
        "      paragraphs = [p.get_text() for span in spans for p in span.find_all(\"p\")]\n",
        "      writeToFile(paragraphs = paragraphs, file = f\"/content/drive/MyDrive/NLP fellowship/week1 tasks/{key}.txt\")\n",
        "    print(f\"file {count} created\")\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "modd8RmbpQNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINISH THE PROGRAM\n",
        "get_page()"
      ],
      "metadata": {
        "id": "TndWvTTCmqer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lEGujr-Xe6_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ASSIGNMENT III** : extract text from 5 pdf documents provided and write them in a single file"
      ],
      "metadata": {
        "id": "UDxjrVRtesoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textract\n"
      ],
      "metadata": {
        "id": "OA5HWiBJfQw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "tsVTYRoC29Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import os\n",
        " import textract, PyPDF2"
      ],
      "metadata": {
        "id": "fziP5lcSejM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "D_iyEaB8WQa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GET THE PDF FILES \n",
        "os.chdir(\"/content/drive/MyDrive/NLP fellowship/files\")\n",
        "PDFs = [file for file in os.listdir() if \"Hansard\" in file]"
      ],
      "metadata": {
        "id": "qOHGZ-9_fhBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PDFs"
      ],
      "metadata": {
        "id": "A7cdsT8DgPXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_firstPage(pdfs):\n",
        "  pages = {}\n",
        "  temp = PyPDF2.PdfFileWriter()\n",
        "  for pdf in pdfs:\n",
        "    pages[pdf] = []\n",
        "\n",
        "  for pdf in pdfs:\n",
        "    reader = PyPDF2.PdfFileReader(pdf)\n",
        "    allowed_pages = reader.getNumPages()\n",
        "    for page_number in range(1, allowed_pages-1):\n",
        "      # pages[pdf].append(reader.getPage(page_number))\n",
        "      temp.add_page(reader.getPage(page_number))\n",
        "\n",
        "  return temp\n"
      ],
      "metadata": {
        "id": "pC2dmiUnw8T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages = remove_firstPage(PDFs)"
      ],
      "metadata": {
        "id": "YLwh-O-2CQXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create template pdf file\n",
        "with open(\"/content/temp.pdf\", \"wb\") as f:\n",
        "  pages.write(f)"
      ],
      "metadata": {
        "id": "pXa92L616VZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTRACT EACH PDF\n",
        "def extract(file, new_destination = \"PDFsExtracted.txt\"):\n",
        "  text = textract.process(file).decode(\"utf-8\")\n",
        "  lines = text.split(\"\\n\")\n",
        "  with open(new_destination, \"a+\") as f:\n",
        "    for line in lines:\n",
        "      if line.strip():\n",
        "        f.write(line+\"\\n\")\n",
        "  with open(new_destination, \"a+\") as f:\n",
        "    f.write(\"\\n\")\n",
        "  \n",
        "extract(\"/content/temp.pdf\")"
      ],
      "metadata": {
        "id": "ftNithcygVnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**END OF THE WEEK ASSIGNMENT** data cleaning "
      ],
      "metadata": {
        "id": "7WUNVRRAgJpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "engKinyFile = \"/content/drive/MyDrive/NLP fellowship/week1 tasks/Eng_Kin-Paralleldata (1).csv\" "
      ],
      "metadata": {
        "id": "e_KHGfRRgTVF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# READ THE FILE\n",
        "df = pd.read_csv(engKinyFile)\n",
        "kiny = df['Kinyarwanda']\n",
        "eng = df[\"English\"]"
      ],
      "metadata": {
        "id": "whfkAuSziGan"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kiny"
      ],
      "metadata": {
        "id": "fvQjroVIzypn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NORMALIZING THE FILE\n",
        "kiny = kiny.str.lower()\n",
        "eng = eng.str.lower()"
      ],
      "metadata": {
        "id": "nOw2g9BUirv9"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE THE WHITE SPACES\n",
        "kiny = kiny.str.strip()\n",
        "eng = eng.str.strip()"
      ],
      "metadata": {
        "id": "CrGmwqYRjvBp"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(kiny)"
      ],
      "metadata": {
        "id": "T00P3ERVxf1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kiny_list = list(kiny)\n",
        "eng_list = list(eng)"
      ],
      "metadata": {
        "id": "wuiZRc4l6mX8"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kiny_list"
      ],
      "metadata": {
        "id": "RZIdOIvrxai4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "punc = string.punctuation\n",
        "print(punc)"
      ],
      "metadata": {
        "id": "T-wdZ2WzTw_S",
        "outputId": "d2973047-d0bc-40ec-8d26-5b4c2959dab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE URLs and EMAILS\n",
        "pattern = re.compile(\"\\w+@[^\\s\\.]{3,}|([a-z]+\\.[\\w]{3,})|([^\\s]+(\\.net|\\.org|\\.com))|www\\..*|htt\\.*|[^\\s]+@[^\\s]+\")\n",
        "found = [pattern.findall(line) for line in eng_list if pattern.findall(line)]\n",
        "EngNo_urls = [pattern.sub(\"\",line) for line in eng_list]\n",
        "KinyNo_urls = [pattern.sub(\"\",line) for line in kiny_list]"
      ],
      "metadata": {
        "id": "eInuwLMwjTc8"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EngNo_urls\n",
        "found\n",
        "# KinyNo_urls"
      ],
      "metadata": {
        "id": "2FnbIizoD0df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE PANCTUATIONS \n",
        "panc_pattern = re.compile(\"[^\\w\\s]+\")\n",
        "EngNo_punc = [panc_pattern.sub(\"\", line) for line in EngNo_urls]\n",
        "KinyNo_punc = [panc_pattern.sub(\"\", line) for line in KinyNo_urls]"
      ],
      "metadata": {
        "id": "_79gSrgJ53wi"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KinyNo_punc"
      ],
      "metadata": {
        "id": "Q1yilcXQ7c-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVE NUMBERS\n",
        "dig_pattern = re.compile(\"\\d+\")\n",
        "EngNo_dig = [dig_pattern.sub(\"\", line) for line in EngNo_punc]\n",
        "KinyNo_dig = [dig_pattern.sub(\"\", line) for line in KinyNo_punc]"
      ],
      "metadata": {
        "id": "j0UZFNzr_Aui"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KinyNo_dig"
      ],
      "metadata": {
        "id": "U2GCqFh3_bZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove english words in kinyarwanda\n",
        "eng_pattern = re.compile(\"\\w+[b-df-hj-np-tv-xz]\\b\")\n",
        "all_engWords = [print(line) for line in KinyNo_dig]"
      ],
      "metadata": {
        "id": "hQol5wlU4hs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_engWords"
      ],
      "metadata": {
        "id": "c929yV9V5fi8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}